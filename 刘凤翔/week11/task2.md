## ğŸ”„ å¤„ç†æµç¨‹å¯¹æ¯”

### 1. çº¯æ–‡æœ¬æé—®å¤„ç†æµç¨‹
```
ç”¨æˆ·æ–‡æœ¬æé—® â†’ æ–‡æœ¬ç†è§£ â†’ å‘é‡æ£€ç´¢ â†’ ç­”æ¡ˆç”Ÿæˆ â†’ è¿”å›çº¯æ–‡æœ¬ç­”æ¡ˆ
```

### 2. æ–‡æœ¬+å›¾ç‰‡æé—®å¤„ç†æµç¨‹  
```
ç”¨æˆ·æ–‡æœ¬+å›¾ç‰‡æé—® â†’ å¤šæ¨¡æ€ç†è§£ â†’ è·¨æ¨¡æ€æ£€ç´¢ â†’ å¤šæ¨¡æ€æ¨ç† â†’ è¿”å›å¯Œåª’ä½“ç­”æ¡ˆ
```

## ğŸ› ï¸ æŠ€æœ¯å®ç°å·®å¼‚

### çº¯æ–‡æœ¬æé—®å¤„ç†ï¼š
```python
async def handle_text_only_question(question: str):
    # 1. æ–‡æœ¬è¯­ä¹‰ç†è§£
    query_embedding = get_text_embedding(question)
    
    # 2. åœ¨æ–‡æœ¬å‘é‡ç©ºé—´æ£€ç´¢
    text_results = vector_db.search(
        query_vector=query_embedding,
        anns_field="text_bge_vector",  # ä½¿ç”¨BGEæ–‡æœ¬ç¼–ç 
        top_k=10
    )
    
    # 3. çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡æ„å»º
    context = build_text_context(text_results)
    
    # 4. æ–‡æœ¬LLMç”Ÿæˆç­”æ¡ˆ
    answer = text_llm.generate(
        question=question,
        context=context
    )
    
    return {
        "answer": answer,
        "supporting_texts": text_results,
        "retrieved_images": []  # å¯èƒ½ä¸ºç©ºæˆ–åŒ…å«ç›¸å…³å›¾ç‰‡
    }
```

### æ–‡æœ¬+å›¾ç‰‡æé—®å¤„ç†ï¼š
```python
async def handle_multimodal_question(question: str, image_base64: str):
    # 1. å¤šæ¨¡æ€è”åˆç†è§£
    image = decode_base64_image(image_base64)
    
    # æ–‡æœ¬ç‰¹å¾æå–
    text_embedding = get_clip_text_features([question])[0]
    
    # å›¾ç‰‡ç‰¹å¾æå–  
    image_embedding = get_clip_image_features([image])[0]
    
    # 2. è·¨æ¨¡æ€æ£€ç´¢ç­–ç•¥
    # ç­–ç•¥A: åˆ†åˆ«æ£€ç´¢åèåˆ
    text_results = vector_db.search(
        query_vector=text_embedding,
        anns_field="text_clip_vector",
        top_k=8
    )
    
    image_results = vector_db.search(
        query_vector=image_embedding, 
        anns_field="image_clip_vector",
        top_k=8
    )
    
    # ç»“æœèåˆå’Œé‡æ’åº
    combined_results = fuse_and_rerank_results(
        text_results, image_results
    )
    
    # 3. å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ„å»º
    multimodal_context = build_multimodal_context(
        text_docs=combined_results.text_docs,
        image_docs=combined_results.image_docs,
        query_image=image
    )
    
    # 4. å¤šæ¨¡æ€LLMç”Ÿæˆç­”æ¡ˆ
    answer = multimodal_llm.generate(
        question=question,
        images=[image] + retrieved_images,  # åŒ…å«æŸ¥è¯¢å›¾ç‰‡å’Œæ£€ç´¢åˆ°çš„å›¾ç‰‡
        context=multimodal_context
    )
    
    return {
        "answer": answer,
        "supporting_texts": combined_results.text_docs,
        "supporting_images": combined_results.image_docs,
        "visual_reasoning": True
    }
```

## ğŸ“Š æ ¸å¿ƒå·®å¼‚åˆ†æ

### 1. **ç‰¹å¾æå–å±‚é¢**
| ç»´åº¦ | çº¯æ–‡æœ¬æé—® | æ–‡æœ¬+å›¾ç‰‡æé—® |
|------|------------|---------------|
| ç‰¹å¾æ¨¡å‹ | BGEæ–‡æœ¬ç¼–ç å™¨ | CLIPå¤šæ¨¡æ€ç¼–ç å™¨ |
| ç‰¹å¾ç©ºé—´ | å•æ¨¡æ€æ–‡æœ¬ç©ºé—´ | å…±äº«å¤šæ¨¡æ€ç©ºé—´ |
| è¯­ä¹‰ç†è§£ | çº¯è¯­è¨€ç†è§£ | è§†è§‰-è¯­è¨€è”åˆç†è§£ |

### 2. **æ£€ç´¢ç­–ç•¥å±‚é¢**
```python
# çº¯æ–‡æœ¬æ£€ç´¢ç­–ç•¥
def text_only_retrieval_strategy(query_text):
    # ä¸»è¦åœ¨æ–‡æœ¬å‘é‡ç©ºé—´æœç´¢
    return search_in_text_space(query_text)

# å¤šæ¨¡æ€æ£€ç´¢ç­–ç•¥  
def multimodal_retrieval_strategy(query_text, query_image):
    strategies = [
        # ç­–ç•¥1: æ–‡æœ¬å¼•å¯¼çš„å›¾ç‰‡æ£€ç´¢
        search_images_by_text(query_text),
        # ç­–ç•¥2: å›¾ç‰‡å¼•å¯¼çš„æ–‡æœ¬æ£€ç´¢  
        search_texts_by_image(query_image),
        # ç­–ç•¥3: å¤šæ¨¡æ€èåˆæ£€ç´¢
        cross_modal_fusion_search(query_text, query_image)
    ]
    return merge_strategies(strategies)
```

### 3. **ä¸Šä¸‹æ–‡æ„å»ºå·®å¼‚**
```python
# çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡
text_context = """
å•†å“A: é«˜æ€§èƒ½ç¬”è®°æœ¬ç”µè„‘ï¼Œé…å¤‡RTXæ˜¾å¡
å•†å“B: è½»è–„åŠå…¬æœ¬ï¼Œç»­èˆªæ—¶é—´é•¿
ç”¨æˆ·é—®é¢˜: {question}
"""

# å¤šæ¨¡æ€ä¸Šä¸‹æ–‡
multimodal_context = """
[å›¾ç‰‡æè¿°] ç”¨æˆ·æä¾›çš„å›¾ç‰‡æ˜¾ç¤ºä¸€å°é»‘è‰²ç¬”è®°æœ¬ç”µè„‘
[ç›¸å…³å•†å“] å•†å“A: ç±»ä¼¼å¤–è§‚çš„æ¸¸æˆæœ¬ (ç›¸ä¼¼åº¦: 0.85)
[ç›¸å…³å•†å“] å•†å“C: ç›¸åŒå“ç‰Œçš„å•†åŠ¡æœ¬ (ç›¸ä¼¼åº¦: 0.78)
ç”¨æˆ·é—®é¢˜: {question}
å›¾ç‰‡ç‰¹å¾: é»‘è‰²ã€é‡‘å±è´¨æ„Ÿã€15.6è‹±å¯¸å±å¹•
"""
```

### 4. **ç”Ÿæˆæ¨¡å‹å·®å¼‚**
```python
# çº¯æ–‡æœ¬ç”Ÿæˆé…ç½®
text_generation_config = {
    "model": "chatglm3-6b",
    "capability": "text_only",
    "input": ["question", "text_context"]
}

# å¤šæ¨¡æ€ç”Ÿæˆé…ç½®  
multimodal_generation_config = {
    "model": "qwen-vl-plus", 
    "capability": "visual_understanding",
    "input": ["question", "query_image", "retrieved_images", "multimodal_context"]
}
```

## ğŸ¯ åº”ç”¨åœºæ™¯å·®å¼‚

### é€‚åˆçº¯æ–‡æœ¬æé—®çš„åœºæ™¯ï¼š
```python
pure_text_scenarios = [
    "è¿™ä¸ªå•†å“æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ",
    "æ¨èå‡ æ¬¾æ€§ä»·æ¯”é«˜çš„ç¬”è®°æœ¬ç”µè„‘",
    "æ¯”è¾ƒAäº§å“å’ŒBäº§å“çš„å·®å¼‚",
    "æ ¹æ®æˆ‘çš„éœ€æ±‚æ¨èå•†å“"  # éœ€æ±‚æè¿°ä¸ºæ–‡æœ¬
]
```

### é€‚åˆæ–‡æœ¬+å›¾ç‰‡æé—®çš„åœºæ™¯ï¼š
```python
multimodal_scenarios = [
    "è¿™ä¸ªå›¾ç‰‡é‡Œçš„ç”µè„‘æ˜¯ä»€ä¹ˆå‹å·ï¼Ÿ",  # æŒ‡ä»£æ€§æé—®
    "å¸®æˆ‘æ‰¾ç±»ä¼¼è¿™ä¸ªå¤–è§‚çš„ç¬”è®°æœ¬ç”µè„‘",  # è§†è§‰ç›¸ä¼¼æ€§æœç´¢
    "è¿™ä¸ªå•†å“çš„è¿™ä¸ªéƒ¨ä»¶æ˜¯åšä»€ä¹ˆç”¨çš„ï¼Ÿ",  # å›¾ç‰‡ä¸­çš„å…·ä½“éƒ¨åˆ†
    "æ ¹æ®æˆ‘å‘çš„å›¾ç‰‡å’Œæˆ‘çš„é¢„ç®—æ¨è",  # å¤šæ¡ä»¶æŸ¥è¯¢
    "è¿™ä¸ªå•†å“çš„é¢œè‰²è¿˜æœ‰å…¶ä»–çš„å—ï¼Ÿ"  # åŸºäºè§†è§‰å±æ€§çš„æŸ¥è¯¢
]
```

## ğŸš€ æ¥å£è®¾è®¡å»ºè®®

åŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘å»ºè®®è¿™æ ·è®¾è®¡é—®ç­”æ¥å£ï¼š

```python
class MultimodalQARequest(BaseModel):
    question: str = Field(..., description="ç”¨æˆ·é—®é¢˜")
    query_image: Optional[str] = Field(None, description="æŸ¥è¯¢å›¾ç‰‡base64")
    modality_preference: str = Field(
        default="auto", 
        description="æ¨¡æ€åå¥½: text_only, visual_heavy, auto"
    )
    search_strategy: str = Field(
        default="adaptive",
        description="æ£€ç´¢ç­–ç•¥: text_centric, visual_centric, cross_modal, adaptive"
    )

class AdaptiveQAHandler:
    async def handle_question(self, request: MultimodalQARequest):
        # è‡ªåŠ¨æ£€æµ‹é—®é¢˜ç±»å‹
        question_type = self.analyze_question_type(
            request.question, 
            request.query_image
        )
        
        if question_type == "text_only":
            return await self.text_centric_qa(request)
        elif question_type == "visual_reference":
            return await self.visual_centric_qa(request)  
        else:  # cross_modal
            return await self.cross_modal_qa(request)
    
    def analyze_question_type(self, question: str, image: Optional[str]):
        # åŸºäºå…³é”®è¯å’Œå›¾ç‰‡å­˜åœ¨æ€§åˆ†æ
        visual_keywords = ["è¿™ä¸ª", "å›¾ç‰‡", "é¢œè‰²", "å¤–è§‚", "ç±»ä¼¼", "æ ·å­"]
        has_visual_ref = any(keyword in question for keyword in visual_keywords)
        
        if image and has_visual_ref:
            return "cross_modal"
        elif image and not has_visual_ref:
            return "visual_centric" 
        elif not image and has_visual_ref:
            return "text_only"  # ä½†å¯èƒ½ä¿¡æ¯ä¸è¶³
        else:
            return "text_only"
```

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–è€ƒè™‘

### çº¯æ–‡æœ¬è·¯å¾„ä¼˜åŒ–ï¼š
- ä½¿ç”¨æ›´è½»é‡çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆBGE-smallï¼‰
- æ–‡æœ¬æ£€ç´¢ç¼“å­˜
- æ‰¹é‡æ–‡æœ¬å¤„ç†

### å¤šæ¨¡æ€è·¯å¾„ä¼˜åŒ–ï¼š
- å›¾ç‰‡ç‰¹å¾é¢„è®¡ç®—å’Œç¼“å­˜
- åˆ†çº§æ£€ç´¢ï¼šå…ˆæ–‡æœ¬ç²—ç­›ï¼Œå†è§†è§‰ç²¾æ’
- å¼‚æ­¥å¹¶è¡Œå¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡ç‰¹å¾

## ğŸª æ€»ç»“

**æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼š**
- **çº¯æ–‡æœ¬æé—®**ï¼šåœ¨è¯­è¨€ç©ºé—´ä¸­è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œæ¨ç†
- **æ–‡æœ¬+å›¾ç‰‡æé—®**ï¼šåœ¨è§†è§‰-è¯­è¨€è”åˆç©ºé—´ä¸­è¿›è¡Œè·¨æ¨¡æ€ç†è§£å’Œæ¨ç†

**æŠ€æœ¯é€‰å‹å»ºè®®ï¼š**
- å¯¹äºçº¯æ–‡æœ¬åœºæ™¯ï¼Œä¼˜å…ˆä½¿ç”¨ä¸“é—¨çš„æ–‡æœ¬æ¨¡å‹ï¼ˆæˆæœ¬ä½ã€æ•ˆæœå¥½ï¼‰
- å¯¹äºå¤šæ¨¡æ€åœºæ™¯ï¼Œéœ€è¦CLIPç­‰è·¨æ¨¡æ€æ¨¡å‹æ”¯æŒ
- å®ç°è‡ªé€‚åº”è·¯ç”±ï¼Œæ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¤„ç†è·¯å¾„

è¿™æ ·çš„è®¾è®¡æ—¢èƒ½ä¿è¯çº¯æ–‡æœ¬æŸ¥è¯¢çš„æ•ˆç‡ï¼Œåˆèƒ½å……åˆ†å‘æŒ¥å¤šæ¨¡æ€æ£€ç´¢çš„ä¼˜åŠ¿