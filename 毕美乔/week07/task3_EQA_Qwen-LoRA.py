# -*- coding: utf-8 -*-
"""eqs_Qwen_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gqZb0ovoMqSNIVBsFkq44D9p_iZCRtM
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer,
)
# pip install peft
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from tqdm import tqdm
import torch


# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
import json

def build_eqa_examples(path, output_path=None):
    """
    å°† sentence.txt + tag.txt è½¬æ¢ä¸º LoRA å¾®è°ƒç”¨çš„ instruction-style NER æ ·æœ¬
    """
    examples = []
    lens = []
    data = json.load(open(path))

    for paragraph in data['data']:
        context = paragraph['paragraphs'][0]['context']
        for qa in paragraph['paragraphs'][0]['qas']:
            # æ„é€  instruction æ ¼å¼
            example = {
                'context': context,
                'question': qa['question'],
                'answers': {
                    'answer_start': [qa['answers'][0]['answer_start']],
                    'text': [qa['answers'][0]['text']]
                }
            }
            lens.append(len(example['context']))
            examples.append(example)

    # ä¿å­˜ä¸º JSON æ ¼å¼
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f_out:
            json.dump(examples, f_out, ensure_ascii=False, indent=2)

        print(f"âœ… å·²ç”Ÿæˆ {len(examples)} æ¡æ ·æœ¬ï¼Œä¿å­˜è‡³ {output_path}")
        print(f"æ–‡æœ¬æœ€é•¿é•¿åº¦ä¸ºï¼š{max(lens)}")

    return examples

def initialize_model_and_tokenizer(model_path):
    """åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹"""
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True,
        local_files_only=True
    )

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦å‡å°‘å†…å­˜å ç”¨
        device_map=None,
        local_files_only=True
    )

    return tokenizer, model

def process_func(example, tokenizer):
    """
    ç”¨äº LoRA å¾®è°ƒå¤§æ¨¡å‹çš„æŠ½å–å¼é—®ç­”æ ·æœ¬é¢„å¤„ç†å‡½æ•°
    æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸Šä¸‹æ–‡ contextã€é—®é¢˜ question å’Œæ ‡å‡†ç­”æ¡ˆ answers
    """

    # ===== 1ï¸âƒ£ ç»„è£… prompt =====
    system_prompt = (
        "<|im_start|>system\n"
        "ä½ æ˜¯ä¸€ä¸ªæŠ½å–å¼é—®ç­”æ¨¡å‹ã€‚è¯·ä»ç»™å®šçš„çŸ¥è¯†æ–‡æœ¬ä¸­ç›´æ¥æŠ½å–æœ€ç¬¦åˆé—®é¢˜çš„ç­”æ¡ˆç‰‡æ®µã€‚\n"
        "å¦‚æœæ— æ³•ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¾“å‡ºï¼šæ— æ³•å›ç­”ã€‚\n"
        "<|im_end|>\n"
    )

    user_prompt = (
        "<|im_start|>user\n"
        f"çŸ¥è¯†æ–‡æœ¬ï¼š{example.get('context', '').strip()}\n"
        f"é—®é¢˜ï¼š{example.get('question', '').strip()}\n"
        "<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    full_prompt = system_prompt + user_prompt

    # ===== 2ï¸âƒ£ ç­”æ¡ˆå¤„ç† =====
    answer_text = example.get("answers", {}).get("text", ["æ— æ³•å›ç­”"])[0].strip()
    if not answer_text:
        answer_text = "æ— æ³•å›ç­”"

    # ===== 3ï¸âƒ£ ç¼–ç è¾“å…¥ =====
    model_inputs = tokenizer(
        full_prompt,
        truncation=False,           # ğŸš« ä¸æˆªæ–­
        padding=False,
        add_special_tokens=False,
        return_attention_mask=True,
    )

    # ===== 4ï¸âƒ£ ç¼–ç è¾“å‡ºï¼ˆtargetï¼‰=====
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            answer_text,
            truncation=False,        # ğŸš« ä¸æˆªæ–­
            padding=False,
            add_special_tokens=False,
        )

    # ===== 5ï¸âƒ£ æ‹¼æ¥è¾“å…¥ + è¾“å‡º + ç»“æŸç¬¦ =====
    input_ids = model_inputs["input_ids"] + labels["input_ids"] + [tokenizer.eos_token_id]
    attention_mask = model_inputs["attention_mask"] + [1] * (len(labels["input_ids"]) + 1)

    # ===== 6ï¸âƒ£ æ„å»º labelsï¼ˆå±è”½è¾“å…¥éƒ¨åˆ†ï¼‰=====
    label_ids = [-100] * len(model_inputs["input_ids"]) + labels["input_ids"] + [tokenizer.eos_token_id]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": label_ids,
    }

def setup_lora(model):
    """è®¾ç½®LoRAé…ç½®å¹¶åº”ç”¨åˆ°æ¨¡å‹"""
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()

    return model

def setup_training_args():
    """è®¾ç½®è®­ç»ƒå‚æ•°"""
    return TrainingArguments(
        output_dir="drive/MyDrive/week07_homework/output_Qwen1.5_eqa",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        logging_steps=100,
        do_eval=True,
        eval_steps=50,
        num_train_epochs=5,
        save_steps=50,
        learning_rate=1e-4,
        save_on_each_node=True,
        gradient_checkpointing=True,
        report_to="none"  # ç¦ç”¨wandbç­‰æŠ¥å‘Šå·¥å…·
    )

def predict(model, tokenizer, example, device='cpu'):
    """æŠ½å–å•ä¸ªæ–‡æœ¬çš„ç­”æ¡ˆ"""

    formatted_text = (
        "<|im_start|>system\n"
        "ä½ æ˜¯ä¸€ä¸ªæŠ½å–å¼é—®ç­”æ¨¡å‹ã€‚è¯·ä»ç»™å®šçš„çŸ¥è¯†æ–‡æœ¬ä¸­ç›´æ¥æŠ½å–æœ€ç¬¦åˆé—®é¢˜çš„ç­”æ¡ˆç‰‡æ®µã€‚\n"
        "å¦‚æœæ— æ³•ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¾“å‡ºï¼šæ— æ³•å›ç­”ã€‚\n"
        "<|im_end|>\n"
        "<|im_start|>user\n"
        f"çŸ¥è¯†æ–‡æœ¬ï¼š{example['context']}\n"
        f"é—®é¢˜ï¼š{example['question']}\n"
        "<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    # Tokenizeè¾“å…¥
    model_inputs = tokenizer([formatted_text], return_tensors="pt").to(device)

    # ç”Ÿæˆé¢„æµ‹
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            do_sample=False,
            temperature=0.2,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response.strip()

def batch_predict(model, tokenizer, examples, device='cuda'):
    """æ‰¹é‡é¢„æµ‹æµ‹è¯•é›†çš„æ„å›¾"""
    pred_labels = []

    for example in tqdm(examples, desc="çŸ¥è¯†é—®ç­”"):
        try:
            pred_label = predict(model, tokenizer, example, device)
            pred_labels.append(pred_label)
        except Exception as e:
            print(f"æŠ½å–ç­”æ¡ˆ '{example}' æ—¶å‡ºé”™: {e}")
            pred_labels.append("")  # å‡ºé”™æ—¶æ·»åŠ ç©ºå­—ç¬¦ä¸²

    return pred_labels

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # 1. åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    # ç¤ºä¾‹è°ƒç”¨
    train_examples = build_eqa_examples("drive/MyDrive/week07_homework/cmrc2018_public/train.json",
                       "drive/MyDrive/week07_homework/cmrc2018_public/train_eqa_instruction_data.json")
    test_examples = build_eqa_examples("drive/MyDrive/week07_homework/cmrc2018_public/dev.json",
                       "drive/MyDrive/week07_homework/cmrc2018_public/test_eqa_instruction_data.json")


    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
    print("åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer...")
    model_path = "drive/MyDrive/week07_homework/models/Qwen/Qwen3-0.6B"
    tokenizer, model = initialize_model_and_tokenizer(model_path)

    # 3. å¤„ç†æ•°æ®
    print("å¤„ç†è®­ç»ƒæ•°æ®...")
    process_func_with_tokenizer = lambda example: process_func(example, tokenizer)
    # 4. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_ds = Dataset.from_list(train_examples[:1000])
    train_tokenized = train_ds.map(process_func_with_tokenizer)
    lengths = [len(x) for x in train_tokenized["input_ids"]]
    import numpy as np

    print("æ ·æœ¬æ•°:", len(lengths))
    print("æœ€å¤§é•¿åº¦:", np.max(lengths))
    print("å¹³å‡é•¿åº¦:", np.mean(lengths))
    print("ä¸­ä½æ•°:", np.median(lengths))
    print("95åˆ†ä½æ•°:", np.percentile(lengths, 95))

    eval_ds = Dataset.from_list(test_examples[-100:])
    eval_tokenized = eval_ds.map(process_func_with_tokenizer)

    # 5. è®¾ç½®LoRA
    print("è®¾ç½®LoRA...")
    model.enable_input_require_grads()
    model = setup_lora(model)

    # 6. é…ç½®è®­ç»ƒå‚æ•°
    print("é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = setup_training_args()

    # 7. åˆ›å»ºTrainerå¹¶å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
        data_collator=DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            padding=True,
            pad_to_multiple_of=8  # ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨
        ),
    )

    trainer.train()

    # 8. ä¿å­˜æ¨¡å‹
    print("ä¿å­˜æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained("drive/MyDrive/week07_homework/output_Qwen1.5_eqa")

def test_examples(examples):
    # ä¸‹è½½æ¨¡å‹
    # modelscope download --model Qwen/Qwen3-0.6B  --local_dir Qwen/Qwen3-0.6B
    model_path = "drive/MyDrive/week07_homework/models/Qwen/Qwen3-0.6B"
    lora_path = "drive/MyDrive/week07_homework/output_Qwen1.5_eqa"
    tokenizer, model = initialize_model_and_tokenizer(model_path)

    # print("Tokenizer path:", tokenizer.name_or_path)
    # print("Tokenizer vocab size:", tokenizer.vocab_size)
    # print("Special tokens:", tokenizer.special_tokens_map)

    # åŠ è½½è®­ç»ƒå¥½çš„LoRAæƒé‡
    model.load_adapter("drive/MyDrive/week07_homework/output_Qwen1.5_eqa/")
    model.cpu()

    # 2ï¸âƒ£ åŠ è½½ LoRA é€‚é…å™¨
    model = PeftModel.from_pretrained(model, lora_path)
    # print(model)

    # 3ï¸âƒ£ ï¼ˆå¯é€‰ï¼‰åˆå¹¶æƒé‡åæ¨ç†
    model = model.merge_and_unload()
    # print(model)
    # print(tokenizer.is_fast)
    # print(tokenizer.total_vocab_size)

    model.eval()
    model = model.float()
    model.cpu()

    # æµ‹è¯•é¢„æµ‹
    for example in examples:
        result = predict(model, tokenizer, example)
        print(f"çŸ¥è¯†æ–‡æœ¬ï¼š{example['context']}\n")
        print(f"é—®é¢˜ï¼š{example['question']}\n")
        print(f"ç­”æ¡ˆ: {result}")

if __name__ == "__main__":

    # æ‰§è¡Œä¸»å‡½æ•°
    result_df = main()

    # æµ‹è¯•
    examples = build_eqa_examples("drive/MyDrive/week07_homework/cmrc2018_public/test.json")
    test_examples(examples[:5])
