# 【task 2】
**对多模态RAG的项目，如果用户使用【文本】提问 vs 【文本 + 图】提问，你会怎么处理？有什么区别？**
# 处理流程
一次 **多模态检索接口** 的真实流程，本质是：

```text
把一个“模态不确定的问题”，拆解成多个“可比较的向量搜索任务”，
再把这些搜索结果重新“拼回知识单元（chunk）”。
```
```
用户请求
  ↓
Query 解析 & 路由
  ↓
多模态向量化
  ↓
多索引检索
  ↓
候选 chunk 聚合
  ↓
重排序 & 过滤
  ↓
返回可解释结果
```
## 第 1 层：Query 解析（不是废话，是关键）
系统真正做的第一件事是：

判断：这个问题“需要哪些感官能力”？

```code
has_text = true
has_image = true
→ multi-modal query
```

📌 这一层常被忽略，但它决定后面所有路径。

## 第 2 层：Query 拆解（不是 1 个查询，而是 N 个）
你以为是：

```
“检索一次”
```
实际是：

```
Query
 ├─ 语义意图（text）
image ─┤
 └─ 视觉语义（image）

```
👉 文本和图像不是融合，而是并行拆解

## 第 3 层：向量化（Embedding 真正发生的事）
1️⃣ 文本向量化
```
text_vec = text_encoder(query_text)
```

它捕捉的是：
  - 概念（风速）
  - 关系（最大）
  - 指代（区域）

2️⃣ 图像向量化
```
image_vec = image_encoder(query_image)
```

它捕捉的是：
  - 颜色分布
  - 结构
  - 形状模式（热力图 / 曲线 / 表格）

📌 这两个向量根本不是一回事

## 第 4 层：多索引检索（真正的“检索”）

这是你最想知道的部分。

系统里实际上有 多个索引
```
Text Index   →  text_embedding → chunk_id
Image Index  → image_embedding → chunk_id
```

并行发生的事情：
文本检索
```
text_hits = search_text_index(text_vec, top_k)
```

返回：
```
[
  { "chunk_id": "c_12", "score": 0.83 },
  { "chunk_id": "c_07", "score": 0.79 }
]
```
图像检索
```
image_hits = search_image_index(image_vec, top_k)
```

返回：
```
[
  { "chunk_id": "c_07", "score": 0.88 },
  { "chunk_id": "c_19", "score": 0.81 }
]
```

📌 注意：两边返回的都是 **chunk_id**
这是整个系统能成立的核心。

## 第 5 层：候选 Chunk 聚合（真正的“多模态融合”）
把所有候选合并：
```
c_07 ← text + image
c_12 ← text only
c_19 ← image only
```
然后做一件非常重要的事：

把“模态得分”映射回“知识单元”
```
chunk_scores = {
  "c_07": fuse(0.79, 0.88),
  "c_12": fuse(0.83, None),
  "c_19": fuse(None, 0.81)
}
```

## 第 6 层：Score Fusion（真正的智能所在）

这一步是：
```
“我该相信文字多一点，还是图像多一点？”
```
典型策略（举例）：
```
final_score = w_text * text_score + w_img * image_score
```

你甚至可以：
```
文本问题 → w_text 高
图像指代问题 → w_img 高
```

📌 这是多模态 RAG 的灵魂部位

## 第 7 层：过滤 & 重排序（不是所有都给 LLM）
系统会做：
  - 去重
  - 低分过滤
  - 文档范围限制
  - 页码连续性校验

最终得到：
```
Top-K Chunk = [c_07, c_12, c_19]
```

## 第 8 层：结构化返回（不是“给模型”，是给系统）

返回给 QA 模块的不是“原始文本”，而是：
```
{
  "chunk_id": "c_07",
  "text": "...",
  "image_ids": ["img_03"],
  "page": 5,
  "document_id": "doc_01"
}
```

📌 这是“可解释性”的基础

## 小结
✅ 1. 多模态检索 ≠ 多模态生成
生成只是最后一步，检索才是关键。

✅ 2. 图文不是混在一起搜的
而是 先拆 → 再并

✅ 3. chunk 是一切的“对齐锚点”
没有 chunk_id，一切都对不上。

# 区别
从以下下三方面陈述产生的差异：
  - ① 提问方式差异 →
  - ② 在 RAG 各阶段产生的真实差异 →
  - ③ 最终结果层面的差异

## 一、两种提问方式的本质对比（输入层）

| 维度   | 纯文本提问      | 文本 + 图像提问                |
| ---- | ---------- | ------------------------ |
| 用户输入 | query_text | query_text + query_image |
| 信息来源 | 语言抽象       | 语言 + 视觉语义                |
| 用户意图 | “我在描述一个概念” | “我在指认/询问一个具体视觉对象”        |
| 歧义风险 | 高（靠语言想象）   | 低（图像锚定）                  |
| 系统目标 | 语义理解       | **语义 + 视觉对齐**            |

👉 **差异的根源在这里**：
是否存在一个**可被模型直接编码的视觉参照物**。 

## 二、在 RAG 各阶段产生的差异（核心重点）

下面是“**系统内部处理差异**”。

### 1 表示（Embedding）阶段差异

| 项目       | 纯文本提问        | 文本 + 图像提问                    |
| -------- | ------------ | ---------------------------- |
| 编码器      | Text Encoder | Text Encoder + Image Encoder |
| Query 向量 | 1 个（text）    | 2 个（text + image）            |
| 向量空间     | 单一语义空间       | **双空间并行**                    |
| 表示能力     | 抽象、概念级       | **概念 + 形态级**                 |

👉 **关键差异**：
文本 + 图像 ≠ “更长的 query”
而是 **多了一条独立的表示通路**

### 2 检索（Retrieval）阶段差异（最关键）

#### 2.1 检索路径对比

  | 项目       | 纯文本提问      | 文本 + 图像提问                |
  | -------- | ---------- | ------------------------ |
  | 检索通路     | Text Index | Text Index + Image Index |
  | 检索方式     | 单路向量搜索     | **双路并行搜索**               |
  | 召回对象     | chunk_id   | chunk_id                 |
  | 是否存在交集判断 | ❌          | ✅（关键）                    |

#### 2.2 「双重命中」机制只在这里出现

  | 情况          | 纯文本提问 | 文本 + 图像提问 |
  | ----------- | ----- | --------- |
  | 文本命中        | ✅     | ✅         |
  | 图像命中        | ❌     | ✅         |
  | 文本 + 图像双重命中 | ❌     | **✅ 独有**  |

**容易忽略的点**：
「文本 + 图像双重命中」
不是一种 chunk 类型（认为chunk 分为 text chunk / image chunk / text+image chunk 三种，事实上，chunk只有一种，text chunk 是它的属性），
而是一种“召回状态”（chunk 在不同检索通路下被同时召回的一种运行时状态）。

### 3 融合 / 重排序（Rerank）阶段差异

| 维度       | 纯文本提问      | 文本 + 图像提问                |
| -------- | ---------- | ------------------------ |
| 打分信号     | text_score | text_score + image_score |
| 额外信号     | 无          | **dual_hit_bonus**       |
| 排序稳定性    | 中          | **高**                    |
| 噪声 chunk | 较多         | 更少                       |

👉 **含图 chunk 在这里开始“碾压纯文本 chunk”**
不是因为“它有图”，而是因为**多信号一致性更强**。

### 4 构造上下文（Context Building）阶段差异

| 项目       | 纯文本提问          | 文本 + 图像提问                |
| -------- | -------------- | ------------------------ |
| 上下文来源    | 主要是 text chunk | text + image-aware chunk |
| 是否包含图片   | 通常不包含          | 通常包含                     |
| chunk 粒度 | 偏抽象            | 偏具体、指向性强                 |
| 上下文歧义    | 可能偏高           | **明显降低**                 |

## 三、最终结果层面的差异（你真正“看到的”）

这是站在用户 / 产品视角的对比。

### 回答质量对比

| 维度   | 纯文本提问  | 文本 + 图像提问    |
| ---- | ------ | ------------ |
| 准确性  | 依赖语言描述 | **更准确**      |
| 指向性  | 容易泛化   | **高度指向具体对象** |
| 幻觉风险 | 相对更高   | **明显更低**     |
| 可解释性 | 一般     | **强（可指图说话）** |

### 适用场景差异

| 场景           | 推荐方式    |
| ------------ | ------- |
| 概念性问题        | 纯文本     |
| 图表 / 架构 / UI | 文本 + 图像 |
| “这个图里的东西是什么” | 文本 + 图像 |
| “流程怎么理解”     | 文本 + 图像 |
| “定义 / 原理”    | 纯文本     |

## 四、一句“系统级总结”

**在多模态 RAG 中，
文本 + 图像提问的优势，不在于“信息更多”，
而在于：
它为检索阶段引入了第二条独立且可验证的语义证据通路，
使 chunk 能够在文本与视觉两个空间中被同时确认，从而显著提升召回置信度与回答稳定性。**
