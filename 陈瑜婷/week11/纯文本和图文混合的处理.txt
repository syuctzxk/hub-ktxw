1. 纯文本提问
用户输入: "RAG技术的核心组件有哪些？"
     ↓
文本Embedding (BGE/CLIP-Text)
     ↓
向量检索 (Milvus)
     ├─→ 检索相关文本块 (BGE向量空间)
     └─→ 检索相关图片 (CLIP向量空间)
     ↓
Rerank重排序
     ↓
拼接Context
     ↓
LLM生成答案 (Qwen-VL)

2.文本 + 图片的提问
用户输入: "这张架构图中的组件是做什么的？" + [图片]
     ↓
     ├─→ 文本Embedding (BGE/CLIP-Text)
     └─→ 图片理解 (Qwen-VL/CLIP-Image)
           - 提取图片特征向量
           - 理解图片语义内容
     ↓
多模态融合策略
     ├─→ 策略1: 加权融合向量
     ├─→ 策略2: 多路检索后合并
     └─→ 策略3: 用VLM先理解图片，生成描述再检索
     ↓
向量检索 (Milvus)
     ├─→ 检索相关文本块
     └─→ 检索相似图片
     ↓
Rerank重排序 (考虑图文相关性)
     ↓
拼接Context (包含用户上传的图片信息)
     ↓
多模态LLM生成答案 (Qwen-VL)
  - 同时理解检索到的文本、图片和用户上传的图片

3.二者的区别
纯文本提问，只需要对输入的文本做编码，然后通过文本和图片的向量检索来回答
文本 + 图片，需要多模态的大模型先理解图片，然后再通过提问的文本编码，理解后的文本编码，图片的编码来做向量的检索
