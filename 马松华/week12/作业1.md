## 一：mcp_server_main.py
```pycon
#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
 Author: Marky
 Time: 2025/11/23 23:19 
 Description:
"""
import asyncio

from fastmcp import FastMCP, Client

from news import mcp as news_mcp
from saying import mcp as saying_mcp
from tool import mcp as tool_mcp

mcp = FastMCP(
    name="MCP-Server"
)


async def setup():
    await mcp.import_server(news_mcp, prefix="")
    await mcp.import_server(saying_mcp,prefix="")
    await mcp.import_server(tool_mcp,prefix="")


async def test_filtering():
    async with Client(mcp) as client:
        tools = await client.list_tools()
        print("Available tools:",[t.name for t in tools])


if __name__ == '__main__':
    asyncio.run(setup())
    asyncio.run(test_filtering())
    mcp.run(transport="sse",port=8900)

```


## 二：Streamlit_demo.py
```pycon
#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
 Author: Marky
 Time: 2025/11/25 21:25 
 Description:
 # 启动命令： streamlit run steamlit_demo.py
 # 启动后链接：http://localhost:8501/
"""
import asyncio

import streamlit as st

from agents import SQLiteSession, AsyncOpenAI, Agent, OpenAIChatCompletionsModel, Runner
from agents.mcp.server import MCPServerSse
from openai.types.responses import ResponseTextDeltaEvent
# 禁用open-ai的trace
from agents import set_default_openai_api, set_tracing_disabled
set_default_openai_api("chat_completions")
set_tracing_disabled(True)


st.set_page_config(page_title="企业职能助手机器人")
session = SQLiteSession("conversation_1")

# 页面的侧边栏
with st.sidebar:
    st.title('职能AI+智能问答')
    if 'API_TOKEN' in st.session_state and len(st.session_state['API_TOKEN']) > 1:
        st.success('API TOKEN已经配置', icon='✅')
        key = st.session_state['API_TOKEN']
    else:
        key = ""

    key = st.text_input('输入Token:', type='password', value=key)
    st.session_state['API_TOKEN'] = key
    model_name = st.selectbox("选择模型", ["qwen-flash", "qwen-max"])
    use_tool = st.checkbox("使用工具")

# 初始化的对话
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "system", "content": "你好，我是企业智能助手，可以AI对话 也可以调用内部工具。"}
    ]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [
        {"role": "system", "content": "你好，我是企业职能助手，可以AI对话 也 可以调用内部工具。"}
    ]


st.sidebar.button('清空聊天', on_click=clear_chat_history)


async def get_model_response(prompt, model_name, use_tool):
    """
    prompt 当前用户输入
    model_name 模型版本
    use_tool 是否调用工具
    """
    async with MCPServerSse(
            name="SSE Python Server",
            params={
                "url": "http://localhost:8900/sse",
            },
            cache_tools_list=False,  # 如果 True 第1次调用后，缓存mcp server 所有工具信息，不再进行list tool
            # tool_filter 对tool筛选（可以写一个函数筛选，也可以通过黑名单/白名单筛选）
            # client_session_timeout_seconds 超时时间
            client_session_timeout_seconds=20
    ) as mcp_server:
        external_client = AsyncOpenAI(
            api_key=key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        if use_tool:
            agent = Agent(
                name="Assistant",
                instructions="",
                mcp_servers=[mcp_server],
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                )
            )
        else:
            agent = Agent(
                name="Assistant",
                instructions="",
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                )
            )

        # session openai-agent 中 缓存的上下文
        result = Runner.run_streamed(agent, input=prompt, session=session)
        async for event in result.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                yield event.data.delta


if len(key) > 1:
    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):  # 用户输入
            st.markdown(prompt)

        with st.chat_message("assistant"):  # 大模型输出
            message_placeholder = st.empty()
            full_response = ""

            with st.spinner("请求中..."):
                try:
                    response_generator = get_model_response(prompt, model_name, use_tool)


                    async def stream_and_accumulate(generator):
                        accumulated_text = ""
                        async for chunk in generator:
                            accumulated_text += chunk
                            message_placeholder.markdown(accumulated_text + "▌")
                        return accumulated_text


                    full_response = asyncio.run(stream_and_accumulate(response_generator))
                    message_placeholder.markdown(full_response)
                except Exception as e:
                    error_message = f"发生错误: {e}"
                    message_placeholder.error(error_message)
                    full_response = error_message
                    print(f"Error during streming: {e}")

            st.session_state.messages.append({"role": "assistant", "content": full_response})

```
