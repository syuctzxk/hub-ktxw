1.如果判断用户使用【纯文本提问】：
    编码：
        使用CLIP模型将查询文本转换为文本向量。
    检索：
        用文本向量检索知识库中相似的图像；
        用文本向量检索知识库中相似的文本。
    结果融合：
        结果融合合并检索结果，并分别计算基础相似度、跨模态一致性分数、空间关系分数，再按 权重1 * 基础相似度 + 权重2 * 跨模态一致性 + 权重3 * 空间关系得到最终得分。
    重排序： 
        最终得分从高到低排序，取top-N个相似检索结果。
    关联性增强：
        检索的结果是文本，找到同文本ID或同文本页码的图像添加到检索结果中。
    生成：
        关联性增强的检索结果与用户查询文本一起给多模态大模型如Qwen-VL大模型，由大模型推理生成答案。

2.如果判断用户使用【文本+图提问】：
    编码：
        使用多模态模型如CLIP分别将文本编码为文本向量、将图像编码为图像向量。
    检索：
        用图像向量检索知识库中相似的图像；
        用图像向量检索知识库中相似的文本；
        用文本向量检索知识库中相似的图像；
        用文本向量检索知识库中相似的文本。
    结果融合：
        结果融合合并检索结果，并分别计算基础相似度、跨模态一致性分数、空间关系分数，再按 权重1 * 基础相似度 + 权重2 * 跨模态一致性 + 权重3 * 空间关系得到最终得分。
    重排序： 
        最终得分从高到低排序，取top-N个相似检索结果。
    关联性增强：
        检索的结果如果是文本，找到同文本ID或同文本页码的图像添加到检索结果中；
        检索的结果如果是图像，找到同文本ID或同图像页码的文本添加到检索结果中。
    生成：
        关联性增强的检索结果与用户查询文本、图像一起给多模态大模型如Qwen-VL大模型，由大模型推理生成答案。
