import json
import re
from typing import Dict, Any, Optional
import httpx
from dataclasses import dataclass
from enum import Enum
from tool_registry import find_most_similar_tool

class EnhancedQwenClientWithMCP:
    def __init__(self):
        self.llm_client = httpx.Client(
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            timeout=30,
            headers={
                "Authorization": f"Bearer {"API-KEY"}",
                "Content-Type": "application/json"
            }
        )
    def process_query(self, query: str) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´æµç¨‹

        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        result = {
            'success': False,
            'tool_name': None,
            'similarity': 0,
            'extracted_params': {},
            'result': None,
            'answer': None,
            'error': None
        }

        try:
            # 1. ä½¿ç”¨RAGæ‰¾åˆ°æœ€åŒ¹é…çš„å·¥å…·
            matched_tool, similarity = find_most_similar_tool(query)

            if not matched_tool or similarity < 0.3:
                result['error'] = "æ²¡æœ‰æ‰¾åˆ°é€‚åˆå¤„ç†æ‚¨é—®é¢˜çš„å·¥å…·ã€‚"
                return result

            result['tool_name'] = matched_tool['name']
            result['similarity'] = similarity

            print(f"ğŸ¯ åŒ¹é…å·¥å…·: {matched_tool['name']} (ç›¸ä¼¼åº¦: {similarity:.3f})")

            # 2. ä½¿ç”¨LLMæå–å‚æ•°
            extracted_params = self.extract_parameters(query, matched_tool)
            result['extracted_params'] = extracted_params

            # 3. è°ƒç”¨å·¥å…·è®¡ç®—
            try:
                func_result = matched_tool['function'](**extracted_params)
                result['result'] = func_result
                result['success'] = True

                # 5. ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”
                answer = self._generate_answer_with_llm(query, matched_tool, extracted_params, func_result)
                result['answer'] = answer

            except Exception as e:
                result['error'] = f"è®¡ç®—é”™è¯¯: {str(e)}"

        except Exception as e:
            result['error'] = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"

        return result

    def extract_parameters(self, query: str, tool_schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨Qwen LLMä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å‚æ•°

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            tool_schema: å·¥å…·çš„å‚æ•°schema

        Returns:
            æå–çš„å‚æ•°å­—å…¸
        """
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‚æ•°æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–æŒ‡å®šå·¥å…·æ‰€éœ€çš„å‚æ•°ã€‚

        è§„åˆ™ï¼š
        1. åªæå–å·¥å…·æ‰€éœ€çš„å‚æ•°ï¼Œä¸è¦æ·»åŠ é¢å¤–çš„å‚æ•°
        2. å¦‚æœç”¨æˆ·æŸ¥è¯¢ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºå‚æ•°å€¼ï¼Œè¯·è®¾ä¸ºnull
        3. ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®ï¼ˆæ•°å­—ã€å­—ç¬¦ä¸²ã€å¸ƒå°”å€¼ç­‰ï¼‰
        4. è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSON
        5. å¯¹äºæ•°å€¼å‚æ•°ï¼Œç¡®ä¿æå–çš„æ˜¯æ•°å­—è€Œä¸æ˜¯æ–‡æœ¬æè¿°

        ç¤ºä¾‹ï¼š
        ç”¨æˆ·æŸ¥è¯¢ï¼š"è®¡ç®—x=3æ—¶çš„äºŒæ¬¡æ–¹ç¨‹å€¼"
        å·¥å…·å‚æ•°ï¼š{"x": "float"}
        è¾“å‡ºï¼š{"x": 3}
        """

        # æ„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = f"""è¯·ä»ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å·¥å…·å‚æ•°ï¼š

        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

        å·¥å…·ä¿¡æ¯ï¼š
        - å·¥å…·åç§°ï¼š{tool_schema.get('name', 'æœªçŸ¥å·¥å…·')}
        - å·¥å…·æè¿°ï¼š{tool_schema.get('description', 'æ— æè¿°')}
        - å‚æ•°å®šä¹‰ï¼š{json.dumps(tool_schema.get('parameters', {}), ensure_ascii=False, indent=2)}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›æå–çš„å‚æ•°ï¼Œæ ¼å¼å¦‚ï¼š{{"å‚æ•°å": å€¼, ...}}
        å¦‚æœæ— æ³•ç¡®å®šå‚æ•°å€¼ï¼Œè®¾ä¸ºnullã€‚
        
        ç›´æ¥è¿”å›JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šã€‚"""

        try:
            # è°ƒç”¨Qwen API
            response = self.llm_client.post(
                "/chat/completions",
                json={
                    "model": "qwen-max",
                    "messages": [{"role":"system","content":system_prompt},{"role":"user","content": user_prompt}],
                    "response_format": {"type": "json_object"},
                    "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                    "max_tokens": 1000
                }
            )

            response.raise_for_status()
            result = response.json()

            # è§£æå“åº”
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "{}")

            # æ¸…ç†å’Œè§£æJSON
            content = content.strip()

            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            # è§£æJSON
            extracted_params = json.loads(content)

            # è½¬æ¢nullä¸ºNone
            for key, value in extracted_params.items():
                if value == "null" or value is None:
                    extracted_params[key] = None

            return extracted_params

        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
            print(f"åŸå§‹å“åº”: {content[:200]}...")
            return {}
        except httpx.HTTPError as e:
            print(f"HTTPè¯·æ±‚é”™è¯¯: {e}")
            return {}
        except Exception as e:
            print(f"æå–å‚æ•°æ—¶å‡ºé”™: {e}")
            return {}

    def _generate_answer_with_llm(self, query: str, tool_info: Dict[str, Any],
                                  params: Dict[str, Any], result: Any) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”"""
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œè®¡ç®—ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€ä¸“ä¸šçš„å›ç­”ã€‚

        å›ç­”è¦æ±‚ï¼š
        1. ç®€æ´æ˜äº†ï¼Œç”¨ä¸­æ–‡å›ç­”
        2. åŒ…å«è®¡ç®—ç»“æœå’Œå‚æ•°ä¿¡æ¯
        3. è§£é‡Šç»“æœçš„æ„ä¹‰
        4. å¦‚æœé€‚ç”¨ï¼Œæä¾›è¿›ä¸€æ­¥å»ºè®®
        5. ä¿æŒå‹å¥½å’Œä¸“ä¸š
        """

        user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›ç­”ï¼š

        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
        
        ä½¿ç”¨çš„å·¥å…·ï¼š{tool_info.get('name', 'æœªçŸ¥å·¥å…·')}
        å·¥å…·æè¿°ï¼š{tool_info.get('description', '')}
        
        è¾“å…¥å‚æ•°ï¼š
        {json.dumps(params, ensure_ascii=False, indent=2)}
        
        è®¡ç®—ç»“æœï¼š{result}
        
        è¯·ç”Ÿæˆä¸€ä¸ªè‡ªç„¶çš„å›ç­”ï¼Œè§£é‡Šè¿™ä¸ªè®¡ç®—è¿‡ç¨‹å’Œç»“æœæ„ä¹‰ã€‚"""

        try:
            # è°ƒç”¨Qwen APIç”Ÿæˆå›ç­”
            response = self.llm_client.client.post(
                "/chat/completions",
                json={
                    "model": "qwen-max",
                    "messages": [{"role":"system","content":system_prompt},{"role":"user","content": user_prompt}],
                    "temperature": 0.7,
                    "max_tokens": 500
                }
            )

            response.raise_for_status()
            result_data = response.json()

            content = result_data.get("choices", [{}])[0].get("message", {}).get("content", "")

            return content.strip()

        except Exception as e:
            print(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            # è¿”å›ç®€å•å›ç­”
            return f"è®¡ç®—ç»“æœä¸º: {result}\nå‚æ•°: {params}"
