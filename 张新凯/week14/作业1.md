# 文档公式解析与智能问答方案理解
### 路线 1：RAG + Qwen Thinking 模型（LLM主导计算）

> 动态生成代码、 动态推理，没有固定的工具范围。

**利用大型语言模型的强大推理能力和上下文理解能力**来完成公式的识别、参数的提取和最终的计算。采用工具调用（Tool-Use/Code Interpreter）模式。让Qwen Thinking模型在识别出公式和参数后，不是自己计算，而是生成一段Python代码（例如使用`sympy`或`numpy`），然后在沙箱环境中执行代码并返回结果。这样可以结合LLM的推理能力和精确计算的优势。

| 优势 (Pros) | 劣势 (Cons) | 适用场景 |
| :--- | :--- | :--- |
| **高度智能与灵活**：Qwen Thinking等高级LLM可以理解复杂的多步推理和上下文依赖，适用于**非标准格式**的文本问答。 | **计算精度难以保证**：LLM在处理浮点运算、高精度计算和符号代数方面**容易出错**，难以达到$10^{-6}$的误差要求。 | 适用于公式的**语义理解、变量定义**、以及需要**多步、定性推理**的场景。 |
| **简化流程**：LLM可以同时处理公式识别（如果输入是$\LaTeX$或文本）、参数抽取和问答生成，减少模块间耦合。 | **成本高昂与延迟**：调用大型闭源模型进行每次计算的成本和延迟较高。 | **不适用于**需要严格、高精度科学计算的场景。 |
| **自然语言交互好**：问答体验更自然流畅，能生成详细的解释。 | **计算验证困难**：计算过程是一个黑箱，难以提供可信的**计算步骤溯源**。 | |

实施步骤：
  - 步骤1：pdf公式的解析，将文档中的公式进行结构解析，并生成对应的$\LaTeX$公式。
  - 步骤2: RAG的检索和排序，用户的提问 与 公式 进行相似度计算，也可以加入rerank 过程，选择得到top1-8待选公式。
  - 步骤3:
    - 使用qwen-3 thinking模型，输入用户提问 + 公式latex ，生成对应的代码 或 sympy ，并执行代码，返回结果。
    - 使用qwen-3 thinking模型，直接推理得到答案。



# 待选方案1

## 一、整体架构概述

这是一个基于RAG（检索增强生成）的**专业领域（理工科）公式计算问答系统**。系统分为两个主要阶段：

### 第一阶段：数据准备与预处理
**data_prepare.py**：`merge_md_by_page_order` 函数
- **功能**：批量处理PDF解析后的Markdown文件
- **输入**：分层结构的目录（每个PDF对应一个子目录，每页对应一个文件）
- **输出**：结构化的Excel表格，每行包含一个PDF的所有页面内容

### 第二阶段：RAG问答系统
**main.py**：基于向量检索和大语言模型的公式计算
- **功能**：根据用户问题，检索相关公式并计算答案
- **核心**：结合嵌入模型检索 + LLM推理计算

## 二、详细流程分析

### 第一阶段流程（数据整理）：
```
输入：PDF解析结果 → 目录结构 → 按页码排序 → 合并内容 → Excel输出
```
1. **目录遍历**：扫描根目录下的所有子目录
2. **文件匹配**：查找 `*_page_*.md` 格式的文件
3. **自然排序**：按文件名中的数字顺序排列（避免"page_10"排在"page_2"前）
4. **内容合并**：将每个子目录的所有页面合并为一个字符串
5. **结构化存储**：保存为Excel，每行对应一个完整文档

### 第二阶段流程（问答系统）：
```
输入：用户问题 → 向量检索 → 提示词构建 → LLM推理 → 答案提取 → 结果保存
```
1. **知识库构建**：
   - 合并多个来源的公式文档（包括第一阶段生成的Excel）
   - 使用Qwen3-Embedding模型将所有文档向量化

2. **问题处理**：
   - 对每个问题进行向量编码
   - 计算与知识库的相似度，检索top-8最相关公式

3. **智能推理**：
   - 构建专业提示词（指定角色、任务、输出格式）
   - 调用大语言模型（Qwen3-235B思维链增强版）进行推理计算
   - 模型需要：选择合适的公式、判断是否可计算、执行计算

4. **结果处理**：
   - 从模型回答中提取JSON格式的数值答案
   - 处理异常情况（无答案、解析失败）
   - 保存为结构化CSV文件

## 三、关键技术特点

### 1. **智能检索策略**
- **嵌入模型**：Qwen3-Embedding-0.6B，专门针对中文优化
- **检索数量**：动态调整（top_k = min(8, corpus_size)）
- **自然排序**：确保文件按逻辑顺序处理

### 2. **大模型应用设计**
- **模型选择**：从32B升级到235B思维链增强版，提升推理能力
- **提示工程**：
  - 明确的角色设定（理工科博士）
  - 详细的规则约束（变量检查、数值要求）
  - 标准化输出格式（JSON）
- **API集成**：使用ModelScope平台，支持国产大模型

### 3. **错误处理机制**
- **多重容错**：
  - 检索失败 → 跳过
  - 解析失败 → 默认值10
  - 无答案 → 特殊标记
- **异常捕获**：对每个处理步骤都进行try-catch包装

### 4. **数据流管理**
```
原始PDF → 解析为Markdown → 按文档合并 → 向量化存储
       ↓
用户问题 → 向量检索 → LLM计算 → 答案提取 → 最终输出
```

## 四、代码亮点与创新

### 亮点：
1. **完整的RAG实现**：从数据准备到问答生成的端到端解决方案
2. **专业领域适配**：针对理工科公式计算场景深度优化
3. **国产模型栈**：完全基于国产大模型（Qwen系列）
4. **工程化实践**：包含进度条、日志输出、批量处理等工程特性

### 创新点：
1. **混合知识源**：结合预解析文档和实时解析PDF结果
2. **智能公式选择**：让LLM在多个相关公式中选择最合适的
3. **容错式设计**：对各类异常情况都有预设处理方案
4. **输出标准化**：强制JSON格式输出，便于后续处理

## 五、潜在问题与改进建议

### 发现的问题：
1. **文件写入错误**：第二段代码中文件打开模式应为'w'而非默认
2. **变量作用域**：异常处理中使用了未定义的变量
3. **硬编码默认值**：答案设为10可能不适合所有场景

### 改进建议：
1. **性能优化**：
   - 向量嵌入可缓存，避免重复计算
   - 批量处理LLM调用，减少API延迟

2. **功能增强**：
   - 添加公式验证机制，确保计算正确性
   - 支持更多输出格式（单位、置信度等）
   - 增加用户交互界面

3. **可靠性提升**：
   - 更健壮的正则表达式提取
   - 多模型投票机制，提高准确率
   - 添加答案验证步骤

# 待选方案2

## 一、整体架构概述

这是一个基于本地部署的大模型（qwen2.5-7b）的**公式计算问答系统**。从已有的代码文件及README来看，系统应该分为三个主要阶段：

### 第一阶段：数据准备与预处理
**read_pdf_or_md.py**：这个脚本的主要功能是：
1. 遍历指定文件夹中的前10个文件
2. 识别PDF和Markdown文件
3. 提取文件内容（PDF按页提取文本，Markdown直接读取）
4. 将文件名和内容保存到DataFrame
5. 输出为CSV文件供后续使用

### 第二阶段：基于文本匹配得到每个问题对应的知识matched.csv
此阶段应该是使用某种文本匹配算法或者RAG等，将原始问题与第一阶段解析到的PDF文件内容进行匹配，得到每个问题最匹配的一个知识。但是此阶段代码未在方案中体现，也许是打包方案代码时出现了遗漏。

### 第三阶段：通过本地部署的大模型（qwen2.5-7b）批量得到答案
**code.py**：这个脚本的主要功能是：
1. 加载本地的Qwen2.5-7B模型
2. 从第二阶段的`matched.csv`读取问题和背景知识对
3. 为每个问题构建提示词，要求模型给出数字答案
4. 使用模型批量生成回答
5. 将回答保存到`content_list2.csv`

## 二、关键技术特点
该方案反映了当前AI开发的几个趋势：
1. **本地化部署**：在本地运行大模型，保障数据安全
2. **RAG架构**：虽然第二份代码未完全实现，但体现了检索增强生成的思路
3. **提示工程**：通过精心设计的提示词控制模型输出
4. **批量处理**：适应企业级大批量文档处理需求
5. **中文本地化**：专门处理中文文档和中文模型


# 待选方案3

## 一、系统架构概述

方案中的两个文件构成了一个完整的**两阶段检索增强生成（RAG）系统**：

### **第一阶段** (`1_run_inference.py`)：
- **任务**：知识检索 + LLM推理
- **输入**：文档库（ZIP）+ 问题集（CSV）
- **输出**：详细推理日志（JSONL）
- **核心**：语义检索 + 结构化提示工程 + 符号计算

### **第二阶段** (`2_generate_submission.py`)：
- **任务**：后处理 + 标准化输出
- **输入**：推理日志（JSONL）
- **输出**：最终提交文件（CSV）
- **核心**：意图分类 + 答案清洗 + 格式标准化

## 二、设计亮点

### 1. **分层处理架构**
- 将复杂问题分解为检索、推理、计算、后处理等多个层次
- 每层专注于单一职责，便于调试和优化
- 松耦合设计允许独立改进各模块

### 2. **健壮性工程**
```python
# 多层异常处理示例
# 第一阶段：计算失败时的多层后备
sympy_result → float(llm_answer) → 0.0

# 第二阶段：解析失败时的多层后备
JSON解析 → 正则提取 → 默认值0.0
```

### 3. **智能意图识别**
- **数值计算类**：精确数学计算为主
- **公式检索类**：识别并返回数学模型
- **结论推导类**：逻辑推理与判断
- 针对不同意图采用差异化处理策略

### 4. **巧妙的数值映射策略**
| 意图类型 | 处理策略 | 输出范围 |
|---------|---------|---------|
| 数值计算 | SymPy精确计算 | 实际计算值 |
| 公式检索 | 成功标志 | 0.0或1.0 |
| 结论推导 | 关键词分析 | -1.0, 0.0, 1.0 |

### 5. **完整的可追溯性**
- 记录了检索到的文档ID
- 保存了LLM原始输出
- 记录了中间计算结果
- 便于问题定位和模型优化

## 三、技术评价

### 优点：

1. **准确性提升**：
   - 检索阶段减少幻觉
   - 符号计算保证数学精确性
   - 后处理规则提升输出一致性

2. **效率平衡**：
   - 离线嵌入计算（一次性）
   - 本地LLM推理（可控制）
   - 批处理优化

3. **可扩展性**：
   - 模块化设计便于添加新文档类型
   - 意图识别可扩展新类别
   - 后处理规则可灵活调整

4. **错误容忍**：
   - 多层异常处理
   - 优雅降级机制
   - 安全默认值保证输出完整性

### 局限性：

1. **依赖本地模型质量**：
   - Sentence Transformer中文模型性能
   - Ollama模型的选择和配置
   - 模型大小和推理速度的权衡

2. **规则化处理的局限性**：
   - 后处理的关键词匹配可能不够精确
   - 意图分类的边界情况处理
   - 复杂问题的多意图交叉

3. **计算资源的挑战**：
   - 大文档库的嵌入计算
   - 高并发问题处理的性能
   - 内存占用优化

## 四、总结
这是一个设计精良、实现严谨的问答系统，特别适合需要高准确性和可解释性的学术或工业应用。虽然在极复杂场景下可能有局限性，但其健壮性设计和渐进式改进框架为持续优化提供了良好基础。