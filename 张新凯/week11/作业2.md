对多模态RAG的项目，如果用户使用【文本】提问 vs 【文本 + 图】提问，在处理流程上会有区别，详细分析如下：

## 1. 纯文本提问处理流程

### 处理流程：
```
用户文本提问 → 文本编码 → 文本向量检索 → 图文关联 → 答案生成
```

### 具体步骤：
```python
# 纯文本提问处理
def process_text_only_query(question: str, kb_id: str):
    # 1. 文本编码和检索
    question_embedding = text_encoder.encode(question)
    text_results = milvus_text.search(question_embedding, top_k=10)
    image_results = milvus_image.search(question_embedding, top_k=5)  # 用文本检索相关图片
    
    # 2. 结果融合和重排序
    combined_results = fuse_text_image_results(text_results, image_results)
    
    # 3. 生成答案
    context = format_context(combined_results)
    answer = qwen_vl.generate(question, context)
    
    return answer, combined_results
```

### 检索特点：
- **文本检索为主**：主要依赖文本相似度匹配
- **图像检索为辅**：用文本query检索相关图片
- **检索范围**：文本chunks + 图片描述/标题

## 2. 文本+图像提问处理流程

### 处理流程：
```
用户(文本+图像)提问 → 多模态编码 → 多模态联合检索 → 跨模态关联 → 答案生成
```

### 具体步骤：
```python
# 多模态提问处理
def process_multimodal_query(question: str, user_images: List, kb_id: str):
    # 1. 多模态特征提取
    text_features = text_encoder.encode(question)
    image_features = []
    for img in user_images:
        img_feat = clip_image_encoder.encode(img)
        image_features.append(img_feat)
    
    # 2. 联合检索策略
    # 策略A: 分别检索后融合
    text_results = milvus_text.search(text_features, top_k=8)
    image_results_from_text = milvus_image.search(text_features, top_k=3)
    
    # 策略B: 用用户图片检索知识库图片
    image_results_from_user_img = []
    for img_feat in image_features:
        results = milvus_image.search(img_feat, top_k=3)
        image_results_from_user_img.extend(results)
    
    # 策略C: 多模态联合嵌入检索
    multimodal_query = fuse_multimodal_features(text_features, image_features)
    multimodal_results = milvus_multimodal.search(multimodal_query, top_k=10)
    
    # 3. 结果融合和重排序
    all_results = fuse_all_results(
        text_results, 
        image_results_from_text,
        image_results_from_user_img, 
        multimodal_results
    )
    
    # 4. 生成答案（包含用户图片上下文）
    context = format_context(all_results, user_images=user_images)
    answer = qwen_vl.generate(question, context, user_images=user_images)
    
    return answer, all_results
```

## 3. 核心区别对比

### 3.1 检索策略差异
| 方面 | 纯文本提问 | 文本+图像提问 |
|------|------------|---------------|
| **查询表示** | 单一文本向量 | 多模态联合表示 |
| **检索目标** | 相关文本+图片描述 | 视觉相似图片+语义相关文本 |
| **相似度计算** | 文本-文本相似度 | 文本-文本 + 图像-图像 + 跨模态相似度 |
| **检索复杂度** | 相对简单 | 复杂，需要多路检索融合 |

### 3.2 上下文构建差异
```python
# 纯文本提问的上下文
context_pure_text = {
    "text_chunks": [chunk1, chunk2, ...],
    "related_images": [img1_info, img2_info, ...],  # 基于文本检索到的图片
    "user_question": question
}

# 多模态提问的上下文  
context_multimodal = {
    "text_chunks": [chunk1, chunk2, ...],
    "related_images": [img1_info, img2_info, ...],
    "user_images": user_images,  # 用户提供的原始图片
    "visual_similar_images": similar_imgs,  # 视觉相似的图片
    "user_question": question,
    "cross_modal_relations": relations  # 跨模态关联信息
}
```

### 3.3 答案生成差异
```python
# 纯文本提问的prompt
prompt_text_only = f"""
基于以下文档内容回答用户问题：
文档内容: {text_chunks}
相关图片描述: {image_descriptions}

用户问题: {question}
请生成答案...
"""

# 多模态提问的prompt  
prompt_multimodal = f"""
基于以下内容回答用户问题：
文档内容: {text_chunks}
相关图片: {image_descriptions}
用户提供的图片: [这里需要模型直接看到用户图片]

请特别关注用户提供的图片与文档内容的关联，
回答时要结合用户图片的具体内容...

用户问题: {question}
"""
```

## 4. 接口设计差异

### 4.1 请求参数扩展
```python
# 纯文本提问接口
POST /api/knowledge-bases/{kb_id}/chat
Request:
{
    "question": "这个设备的功能是什么？",
    # ... 其他参数
}

# 多模态提问接口  
POST /api/knowledge-bases/{kb_id}/chat
Request: multipart/form-data
- question: "这个设备的功能是什么？"
- images: [file1, file2, ...]  # 用户上传的图片
- multimodal_config: {
    "retrieval_strategy": "joint|separate|hybrid",
    "image_weight": 0.6,      # 图像检索权重
    "text_weight": 0.4,       # 文本检索权重
    "cross_modal_fusion": True
}
```

### 4.2 响应内容扩展
```python
# 多模态提问的响应扩展
Response:
{
    "answer": "根据您提供的图片和文档内容，这个设备是...",
    "sources": [
        # 常规来源
        {
            "type": "text",
            "content": "文本片段...",
            "document_id": "doc1",
            "page_num": 5
        },
        # 用户图片相关的特殊来源
        {
            "type": "user_image_relation",
            "user_image_index": 0,  # 用户提供的第几张图片
            "related_content": "与用户图片关联的文档内容",
            "similarity_score": 0.85,
            "relation_type": "visual_similar|semantic_related"
        }
    ],
    "multimodal_insights": {
        "user_images_analyzed": 2,
        "visual_matches_found": 3,
        "cross_modal_relations": 5
    }
}
```

## 5. 技术挑战和解决方案

### 5.1 多模态提问的挑战
- **计算复杂度高**：多路检索和特征融合
- **跨模态对齐**：确保文本和图像语义对齐
- **结果融合策略**：如何平衡不同检索路径的结果

### 5.2 解决方案
```python
# 优化策略
def optimize_multimodal_retrieval(question, user_images):
    # 1. 基于问题类型选择策略
    if is_visual_dominant_question(question):
        # 视觉主导问题，优先图像检索
        strategy = "image_first"
    elif is_text_dominant_question(question):
        # 文本主导问题，优先文本检索  
        strategy = "text_first"
    else:
        # 平衡策略
        strategy = "balanced"
    
    # 2. 并行检索优化
    with ThreadPoolExecutor() as executor:
        text_future = executor.submit(retrieve_text, question)
        image_future = executor.submit(retrieve_images, user_images)
        # ... 其他检索任务
    
    # 3. 动态权重调整
    weights = calculate_dynamic_weights(question, user_images)
    
    return fuse_with_strategy(results, strategy, weights)
```

## 总结

**纯文本提问**更侧重于语义匹配和文本推理，而**文本+图像提问**需要复杂的跨模态理解和多路检索融合。多模态提问在检索精度、答案相关性方面有潜在优势，但同时也带来了更高的计算复杂度和技术挑战。

在实际实现中，可以考虑：
1. 提供两种接口模式，根据输入自动路由
2. 实现渐进的检索策略，从简单到复杂
3. 增加缓存机制优化多模态检索性能
4. 提供详细的检索过程解释，增强可解释性